{"cells":[{"cell_type":"code","source":["import sys\nmodule_path = '/dbfs/spark/stocksETL/script/'\nif module_path not in sys.path:\n  sys.path.insert(0,'/dbfs/spark/stocksETL/script/')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66025f9b-aa9c-4fe5-ab78-d925a0888c28"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import os\nimport json\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as func\nfrom pyspark.sql.types import StructType,StructField,DateType\nfrom pyspark.sql.types import TimestampType,StringType,IntegerType,DecimalType\nimport psycopg2\nfrom random import randint\nimport jobTracker as t"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f580eeaa-b6ca-495d-9c49-2f25e0580579"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Database connection details\n\ndbutils.widgets.text(\"DB_NAME\", \"\",\"\")\ndbName = dbutils.widgets.get(\"DB_NAME\")\n\ndbutils.widgets.text(\"DB_HOST\", \"\",\"\")\ndbHost = dbutils.widgets.get(\"DB_HOST\")\n\ndbutils.widgets.text(\"DB_USER\", \"\",\"\")\ndbUser = dbutils.widgets.get(\"DB_USER\")\n\ndbutils.widgets.text(\"DB_PWD\", \"\",\"\")\ndbPwd = dbutils.widgets.get(\"DB_PWD\")\n\ndbutils.widgets.text(\"DB_PORT\", \"\",\"\")\ndbPort = dbutils.widgets.get(\"DB_PORT\")\n\n#  Getting blob connection details\n\ndbutils.widgets.text(\"storage_acct\", \"\",\"\")\nstorage_acct = dbutils.widgets.get(\"storage_acct\")\n\ndbutils.widgets.text(\"container_name\", \"\",\"\")\ncontainer_name = dbutils.widgets.get(\"container_name\")\n\ndbutils.widgets.text(\"blob_key\", \"\",\"\")\nblob_key = dbutils.widgets.get(\"blob_key\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"599d64fa-a9a0-4f70-9698-10eee0a55591"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["mountName = \"stocksETL\"\nmounts = [str(i) for i in dbutils.fs.ls('/mnt/')] \n\nif \"FileInfo(path='dbfs:/mnt/\" + mountName + \"/', name='\" + mountName + \"/', size=0)\" in mounts:\n  print(\"mount already created\")\nelse:\n  dbutils.fs.mount(\n    source = \"wasbs://\" + container_name + \"@\" + storage_acct +\".blob.core.windows.net\",\n    mount_point = \"/mnt/stocksETL\",\n    extra_configs = {\"fs.azure.account.key.\" + storage_acct + \".blob.core.windows.net\":blob_key})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4538a008-4902-472a-a529-e71bf266f060"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">mount already created\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">mount already created\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def parse_csv(line):\n    \n    record_type_pos = 2\n    record = line.split(\",\") \n    \n    try:        \n        # [logic to parse records]\n        if record[record_type_pos] == \"T\":\n            event = [record[0],record[1],record[2],record[3],record[4],int(record[5]),record[6],\\\n                    Decimal(record[7]),int(record[8]),Decimal(record[9]),int(record[10]),\"T\"]\n            return event\n        elif record[record_type_pos] == \"Q\":\n            event = [record[0],record[1],record[2],record[3],record[4],int(record[5]),record[6],\\\n                    Decimal(record[7]),int(record[8]),Decimal(record[9]),int(record[10]),\"Q\"]\n            \n            return event\n    except Exception as e:      \n        event = [None,None,record[record_type_pos],None,None,None,None,None,None,None,None,\"B\"]\n        return event\n    \ndef parse_json(line):\n    \n    record = json.loads(line)\n    record_type = record['event_type']  \n\n    try:                           \n        # [logic to parse records]\n        if record_type == \"T\":            \n            event = [record['trade_dt'],record['file_tm'],record['event_type'],record['symbol'],\\\n                record['event_tm'],int(record['event_seq_nb']),record['exchange'],Decimal(record['bid_pr']),\\\n                    int(record['bid_size']),Decimal(record['ask_pr']),int(record['ask_size']),\"T\"]\n            return event\n        elif record_type == \"Q\":\n            event = [record['trade_dt'],record['file_tm'],record['event_type'],record['symbol'],\\\n                record['event_tm'],int(record['event_seq_nb']),record['exchange'],Decimal(record['bid_pr']),\\\n                int(record['bid_size']),Decimal(record['ask_pr']),int(record['ask_size']),\"Q\"]\n            return event\n    except Exception as e:\n        event = [None,None,record['event_type'],None,None,None,None,None,None,None,None,\"B\"]\n        return event"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2751462a-7ae9-44ca-8c58-779fea619cd4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def main():\n    \n    try:\n        csv_input_file_path = \"/mnt/stocksETL/spark/inputfiles/data/csv/\"\n        json_input_file_path = \"/mnt/stocksETL/spark/inputfiles/data/json/\"\n        output_path = \"/mnt/stocksETL/spark/outputfiles\"\n        \n        jobTrack = t.Tracker(\"stockETL1\")\n        jobId = jobTrack.assign_job_id()\n        dbConn = jobTrack.get_db_connection(dbName,dbHost,dbUser,dbPwd,dbPort)\n        \n        sc = spark.sparkContext\n        \n        # Define the common schema for both CSV and JSON files\n        common_event = StructType([StructField(\"trade_dt\", StringType(),True),\\\n                                    StructField(\"arrival_tm\", StringType(),True),\\\n                                    StructField(\"rec_type\", StringType(),True),\\\n                                    StructField(\"symbol\", StringType(),True),\\\n                                    StructField(\"event_tm\", StringType(),True),\\\n                                    StructField(\"event_seq_nb\", IntegerType(),True),\\\n                                    StructField(\"exchange\", StringType(),True),\\\n                                    StructField(\"bid_pr\", DecimalType(),True),\\\n                                    StructField(\"bid_size\", IntegerType(),True),\\\n                                    StructField(\"ask_pr\", DecimalType(),True),\\\n                                    StructField(\"ask_size\", IntegerType(),True),\\\n                                    StructField(\"partition\", StringType(),True)])\n        \n        job_sts = \"processing csv files\"\n        jobTrack.update_job_status(jobId,job_sts,dbConn)\n        \n        # Processing CSV input files\n        filepath = csv_input_file_path\n\n        raw = sc.textFile(filepath)\n        parsed = raw.map(lambda line: parse_csv(line))\n\n        data = spark.createDataFrame(parsed,common_event)\n        out_path = output_path\n        data.write.partitionBy(\"partition\").mode(\"append\").parquet(out_path)\n\n        job_sts = \"processing JSON files\"\n        jobTrack.update_job_status(jobId,job_sts,dbConn)\n        \n        # Processing JSON output files\n        filepath = json_input_file_path\n\n        raw = sc.textFile(filepath)\n        parsed = raw.map(lambda line: parse_json(line))\n\n        job_sts = \"writing output to blob folder\"\n        jobTrack.update_job_status(jobId,job_sts,dbConn)\n        \n        data = spark.createDataFrame(parsed,common_event)\n        out_path = output_path\n        data.write.partitionBy(\"partition\").mode(\"append\").parquet(out_path)\n        \n        dbConn.commit()\n        dbConn.close()\n\n    except Exception as e:\n        print(str(e))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f0da632-7b94-4cb5-8a93-e459586a59fa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["if __name__ == \"__main__\":\n    main()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2f54179-0d4b-413b-9a60-9b5dc6d2dbbd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["main()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"745e5245-14b9-47f1-9b87-0fc2c4af60fe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"stocksETL1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{"DB_PWD":{"nuid":"0874623d-b0f3-4498-8e40-587d747cd12a","currentValue":"Password123$","widgetInfo":{"widgetType":"text","name":"DB_PWD","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"DB_NAME":{"nuid":"0239ed69-d6d5-41ea-b1d7-b1f017e89537","currentValue":"postgres","widgetInfo":{"widgetType":"text","name":"DB_NAME","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"storage_acct":{"nuid":"815ef050-0f7f-45b0-b40d-7deb5db1cb5a","currentValue":"sparketlstocks","widgetInfo":{"widgetType":"text","name":"storage_acct","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"DB_HOST":{"nuid":"b24f7fc8-89fa-4e5a-9a72-fe1ab9fc6b4c","currentValue":"sparklog.postgres.database.azure.com","widgetInfo":{"widgetType":"text","name":"DB_HOST","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"container_name":{"nuid":"f86e41dd-8bc4-427d-8ab9-d51971128fb3","currentValue":"stocks","widgetInfo":{"widgetType":"text","name":"container_name","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"DB_USER":{"nuid":"61b96411-6d9e-43a2-9864-ba2a49abd25a","currentValue":"adminusr@sparklog","widgetInfo":{"widgetType":"text","name":"DB_USER","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"blob_key":{"nuid":"9cccc8a9-5b08-4638-b5e9-02bb86e80a3e","currentValue":"ZENoQ7597TTQhXIT5gM2f6VS","widgetInfo":{"widgetType":"text","name":"blob_key","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"DB_PORT":{"nuid":"72d5aca5-3949-4455-b07c-98177fb803ee","currentValue":"","widgetInfo":{"widgetType":"text","name":"DB_PORT","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":2641078628501534}},"nbformat":4,"nbformat_minor":0}
