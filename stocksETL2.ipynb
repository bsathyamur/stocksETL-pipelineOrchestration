{"cells":[{"cell_type":"code","source":["import sys\nmodule_path = '/dbfs/spark/stocksETL/script/'\nif module_path not in sys.path:\n  sys.path.insert(0,'/dbfs/spark/stocksETL/script/')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c79e3fe-6f2c-44aa-be1b-b16a1213c715"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import os\nimport json\nimport datetime\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as func\nfrom pyspark.sql.types import StructType,StructField,DateType,TimestampType,StringType,IntegerType,DecimalType\nfrom pyspark.sql.functions import year, month, dayofmonth,to_timestamp,to_date,split,substring,col,when\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.functions import lit,split,concat,ceil\nfrom pyspark.sql.window import Window"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d4b98aa-ca85-4d11-bf9b-885e84c204df"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import jobTracker as track"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"036e6542-ced3-44f8-92f8-f9a9fd185ee6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Database connection details\n\ndbutils.widgets.text(\"DB_NAME\", \"\",\"\")\ndbName = dbutils.widgets.get(\"DB_NAME\")\n\ndbutils.widgets.text(\"DB_HOST\", \"\",\"\")\ndbHost = dbutils.widgets.get(\"DB_HOST\")\n\ndbutils.widgets.text(\"DB_USER\", \"\",\"\")\ndbUser = dbutils.widgets.get(\"DB_USER\")\n\ndbutils.widgets.text(\"DB_PWD\", \"\",\"\")\ndbPwd = dbutils.widgets.get(\"DB_PWD\")\n\ndbutils.widgets.text(\"DB_PORT\", \"\",\"\")\ndbPort = dbutils.widgets.get(\"DB_PORT\")\n\ndbutils.widgets.text(\"storage_acct\", \"\",\"\")\nstorage_acct = dbutils.widgets.get(\"storage_acct\")\n \ndbutils.widgets.text(\"container_name\", \"\",\"\")\ncontainer_name = dbutils.widgets.get(\"container_name\")\n \ndbutils.widgets.text(\"blob_key\", \"\",\"\")\nblob_key = dbutils.widgets.get(\"blob_key\")\n \ndbutils.widgets.text(\"curr_date\", \"\",\"\")\ncurr_date = dbutils.widgets.get(\"curr_date\")\n \ndbutils.widgets.text(\"prev_date\", \"\",\"\")\nprev_date = dbutils.widgets.get(\"prev_date\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db312aa5-6219-4500-bf99-55ef89b2f0e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["mountName = \"stocksETL\"\nmounts = [str(i) for i in dbutils.fs.ls('/mnt/')] \n \nif \"FileInfo(path='dbfs:/mnt/\" + mountName + \"/', name='\" + mountName + \"/', size=0)\" in mounts:\n  print(\"mount already created\")\nelse:\n  dbutils.fs.mount(\n    source = \"wasbs://\" + container_name + \"@\" + storage_acct +\".blob.core.windows.net\",\n    mount_point = \"/mnt/stocksETL\",\n    extra_configs = {\"fs.azure.account.key.\" + storage_acct + \".blob.core.windows.net\":blob_key})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccb35298-430a-44be-bcca-a243943d00ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">mount already created\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">mount already created\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def main():\n    \n    try:\n \n        output_path = \"/mnt/stocksETL/spark/outputfiles/\"\n \n        jobTrack = track.Tracker(\"stockETL2\")\n        jobId = jobTrack.assign_job_id()\n        dbConn = jobTrack.get_db_connection(dbName,dbHost,dbUser,dbPwd,dbPort)\n      \n        # Reading the corrected output for the trade files\n \n        trade_path = output_path + 'partition=T'\n        trade_common = spark.read.parquet(trade_path)\n \n        trade = trade_common.select(\"trade_dt\",\"symbol\",\"exchange\",\"event_tm\",\"event_seq_nb\",\\\n                                \"arrival_tm\",\"bid_pr\")\n \n        job_sts = \"reading corrected output files for trade\"\n        jobTrack.update_job_status(jobId,job_sts,dbConn)\n  \n        # Renaming bid_pr to trade_pr\n \n        trade = trade.withColumnRenamed(\"bid_pr\",\"trade_pr\")\n \n        trade_cnt = trade.groupBy('trade_dt','symbol','exchange',\\\n            'event_tm','event_seq_nb',\"trade_pr\").agg(func.max('arrival_tm').alias(\"max_arrival_dtm\"))\n \n        #Join and filter the records for latest transaction\n \n        trade.createOrReplaceTempView(\"trade_tbl\")\n        trade_cnt.createOrReplaceTempView(\"trade_cnt_tbl\")\n \n        join_trade_df = spark.sql(\"\"\"\n            select * \n            from trade_tbl t1\n            where EXISTS (\n                    select 1 from trade_cnt_tbl t2\n                    WHERE t1.trade_dt = t2.trade_dt\n                        and t1.symbol = t2.symbol\n                        and t1.exchange = t2.exchange\n                        and t1.event_tm = t2.event_tm\n                        and t1.event_seq_nb = t2.event_seq_nb\n                        and t1.trade_pr = t2.trade_pr\n                        and t1.arrival_tm = t2.max_arrival_dtm\n                    )\n            \"\"\")\n \n        join_trade_df.createOrReplaceTempView(\"join_trade_df\")\n \n        job_sts = \"write corrected output files to blob\"\n        jobTrack.update_job_status(jobId,job_sts,dbConn)\n    \n        # Write the corrected data to the blob\n \n        trade_dates = spark.sql(\"\"\"\n        select distinct trade_dt from join_trade_df\n        \"\"\")\n \n        trade_output_path = output_path\n \n        for t in trade_dates.collect(): \n            trade_date = t.trade_dt\n \n            spark_sql_stmt = \"select * from join_trade_df where trade_dt ='\" + trade_date +\"'\"\n            trade_dt_df = spark.sql(spark_sql_stmt)\n \n            trade_dt_df.write.mode(\"overwrite\").\\\n                parquet(trade_output_path + \"trade/trade_dt={}\".format(trade_date))\n            \n        trade_path = output_path + \"trade/trade_dt=\" + curr_date\n \n        df = spark.read.parquet(trade_path)\n \n        df.createOrReplaceTempView(\"tmp_trade_moving_avg\")\n            \n        job_sts = \"calculating moving average\"\n        jobTrack.update_job_status(jobId,job_sts,dbConn)\n        \n        # Calculating the moving average for last 30 minutes\n        mov_avg_df = spark.sql(\"\"\"select symbol, exchange, trade_dt,event_tm, event_seq_nb, trade_pr,\n                            AVG(trade_pr) OVER(PARTITION BY (symbol) ORDER BY CAST(event_tm AS timestamp) \n                            RANGE BETWEEN INTERVAL 30 MINUTES PRECEDING AND CURRENT ROW) as mov_avg_pr\n                            from tmp_trade_moving_avg\"\"\")       \n \n        # DROP HIVE TABLE IF EXISTS\n        spark.sql(\"DROP TABLE IF EXISTS temp_trade_moving_avg\")\n \n        # Save the data frame as table in hive\n        mov_avg_df.write.saveAsTable(\"temp_trade_moving_avg\")\n        \n        # Loading the previous date to a dataframe (df)\n \n        trade_path = output_path + \"trade/trade_dt=\" + prev_date\n \n        # read the parquet files from the output path\n        \n        df = spark.read.parquet(trade_path)\n \n        df.createOrReplaceTempView(\"tmp_last_trade\")\n \n        # Calculating the moving average for last 30 minutes\n        last_pr_df = spark.sql(\"\"\"select symbol, trade_dt,exchange, event_tm, event_seq_nb, trade_pr,\n                                AVG(trade_pr) OVER(PARTITION BY (symbol) ORDER BY CAST(event_tm AS timestamp) \n                                RANGE BETWEEN INTERVAL 30 MINUTES PRECEDING AND CURRENT ROW) as last_pr\n                                from tmp_last_trade\"\"\")\n \n \n        # DROP HIVE TABLE IF EXISTS\n        spark.sql(\"DROP TABLE IF EXISTS temp_last_trade\")\n \n        # Save the data frame as table in hive\n        last_pr_df.write.saveAsTable(\"temp_last_trade\")\n        \n        job_sts = \"Loading quotes data\"\n        jobTrack.update_job_status(jobId,job_sts,dbConn)\n        \n        # Load the quotes data\n \n        quotes_path = output_path \n \n        quotes = spark.read.parquet(quotes_path + \"partition=Q\")\n \n        quotes.createOrReplaceTempView(\"quotes\")\n \n        quote_union = spark.sql(\"\"\"\n                SELECT trade_dt,rec_type,symbol,event_tm,event_seq_nb,exchange,bid_pr,bid_size,ask_pr,\n                ask_size,null as trade_pr,null as mov_avg_pr FROM quotes\n                UNION\n                SELECT trade_dt,\"T\" as rec_type,symbol,event_tm,event_seq_nb,exchange,null as bid_pr,null as bid_size,null as ask_pr,\n                null as ask_size,trade_pr,mov_avg_pr FROM temp_trade_moving_avg\n                \"\"\")\n \n        quote_union.createOrReplaceTempView(\"quote_union\")\n \n        quote_union_update = spark.sql(\"\"\"\n                select *,\n                last_value(trade_pr,true) OVER(PARTITION BY (symbol) \n                ORDER BY CAST(event_tm AS timestamp) DESC) as last_trade_pr,\n                last_value(mov_avg_pr,true) OVER(PARTITION BY (symbol) \n                ORDER BY CAST(event_tm AS timestamp) DESC) as last_mov_avg_pr\n                from quote_union\n                \"\"\")\n \n        quote_union_update.createOrReplaceTempView(\"quote_union_update\")\n \n        quote_update = spark.sql(\"\"\"\n                select trade_dt, symbol, event_tm, event_seq_nb, exchange,\n                bid_pr, bid_size, ask_pr, ask_size, last_trade_pr, last_mov_avg_pr\n                from quote_union_update\n                where rec_type = 'Q'\n                \"\"\")\n \n        quote_update.createOrReplaceTempView(\"quote_update\")\n \n        quote_final = spark.sql(\"\"\"\n                select trade_dt, A.symbol, event_tm, event_seq_nb, exchange,\n                bid_pr, bid_size, ask_pr, ask_size, last_trade_pr, last_mov_avg_pr,\n                bid_pr - close_pr as bid_pr_mv, ask_pr - close_pr as ask_pr_mv\n                from quote_update A LEFT OUTER JOIN (select distinct symbol,last_pr as close_pr from temp_last_trade) B\n                on A.symbol = B.symbol\n                \"\"\")    \n \n        out_path = output_path \n \n        job_sts = \"writing quotes data to blob\"\n        jobTrack.update_job_status(jobId,job_sts,dbConn)\n    \n        trade_date = curr_date\n        quote_final.write.mode(\"overwrite\").parquet( out_path + \"quote-trade-analytical/date={}\".format(trade_date))        \n \n        dbConn.commit()\n        dbConn.close()\n    \n    except Exception as e:\n        print(str(e))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"894cf994-4463-4305-b4bc-a38e56b90953"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["if __name__ == \"__main__\":\n    main()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d34956dd-e1a7-4b98-9aee-1cd3067ed95d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["main()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d78bb788-c114-4664-a979-490609a5f418"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"stocksETL2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{"DB_PWD":{"nuid":"1c6b4c63-613c-46b9-bf30-fd5adbb0ef81","currentValue":"Password123$","widgetInfo":{"widgetType":"text","name":"DB_PWD","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"DB_NAME":{"nuid":"46be0bb9-2574-49e3-99fd-0cd928358664","currentValue":"postgres","widgetInfo":{"widgetType":"text","name":"DB_NAME","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"storage_acct":{"nuid":"a685d252-a582-4848-b81a-061ad680c86e","currentValue":"sparketlstocks","widgetInfo":{"widgetType":"text","name":"storage_acct","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"DB_HOST":{"nuid":"de3cf82f-23c2-4adc-9ae5-3ff365a892d0","currentValue":"sparklog.postgres.database.azure.com","widgetInfo":{"widgetType":"text","name":"DB_HOST","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"container_name":{"nuid":"ebb85837-dae7-496a-a2c9-02dbd9008fb0","currentValue":"stocks","widgetInfo":{"widgetType":"text","name":"container_name","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"prev_date":{"nuid":"1c98c948-d99b-4d59-8f33-b3043bc57959","currentValue":"2020-08-05","widgetInfo":{"widgetType":"text","name":"prev_date","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"curr_date":{"nuid":"f0014689-6ebd-41a1-bb18-6d0bb281ff8e","currentValue":"2020-08-06","widgetInfo":{"widgetType":"text","name":"curr_date","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"DB_USER":{"nuid":"3d7aecd8-b18e-4171-b7ad-2d6d221294fe","currentValue":"adminusr@sparklog","widgetInfo":{"widgetType":"text","name":"DB_USER","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"blob_key":{"nuid":"f5228e98-7866-4f2e-8312-138fb5a2054d","currentValue":"ZENoQ7597TTQhXIT5gM2f6VS","widgetInfo":{"widgetType":"text","name":"blob_key","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"DB_PORT":{"nuid":"f3581057-a9fa-45e2-9827-33d17d0e9463","currentValue":"5432","widgetInfo":{"widgetType":"text","name":"DB_PORT","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":2364477861478415}},"nbformat":4,"nbformat_minor":0}
